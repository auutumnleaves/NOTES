李宏毅深度学习，课程链接：

https://www.bilibili.com/video/BV1J94y1f7u5/?spm_id_from=333.337.search-card.all.click&vd_source=18aa3116351a43976d5dcc44c5980e00

---

对于入门，重点要看的内容以及看的顺序如下：

>**加粗**表示已经看过的，大概能明白的
><u>下划线</u>表示看过但看不太懂、和没看区别不大的
>其他表示还没看过的

**1.机器学习基础概念：p2-p3-p7** 

**2.Pytorch教程**-不用看这里的视频，去看B站刘二大人的视频 

**3.反向传播基础知识：p8** 

<u>4.案例讲解回归问题：p9 </u>

<u>5.案例讲解分类问题: p10-p11 </u>

**6.梯度下降算法: p18-p19**【新的优化器讲解可以先不看】

**7.卷积神经网络:p22**

**8.RNN: p28**-<u>p29 </u>

**9.自注意力机制: p26-p27**

10.Transformer: p32-p33 

11.BERT: P47-p48-p50-p51

---

**【说明】**

**第1讲：**

深度学习介绍：主要是基础概念的介绍，快速过一遍。 

**第2讲：**

为什么训练网络会失败：主要是将训练网络的一些细节，局部最小值，鞍点，自适应学习率，损失函数等。这一讲的选修的梯度下降必看，新的优化器可以先不看，如果有余力可以看，主要讲了对梯度下降的一些改进。 

**第3讲：**

图像作为输入：CNN网络，必看，非常重要 

**第4讲：**

序列作为输入：先看选修的RNN，再去看自注意力机制，不要搞错顺序。因为注意力太火了，所以RNN放在了选修，不过我认为还是要先看RNN模型基础，再去看自注意力机制，为下面的Transformer模型做准备。选修中的GNN网络，可以你自己的需求，入门阶段可以先跳过不看。 

**第5讲：**

序列到序列：主要讲了Transformer模型，必看，选修的指针网络可以先不看。 

**第6讲：**

生成模型：主要是对GAN理论的介绍。看你自己研究方向，如果是GAN方向的，可以细细看下，如果入门选手，直接跳过。 

**第7讲：**

自监督学习，必看，很火。主要看关于BERT介绍相关的视频，比如模型介绍，微调，预训练等。BERT的各种变体比如Spanbert等可以先不看，同理GPT有余力就看，没余力直接跳过，不影响。 

**第8讲：**

自编码，可以先不看，取决于你的自己研究方向。

第9，10，11，12，13，14，15讲，入门阶段可以先不看【取决于你的自己研究方向】，偏理论。入门之后，再来看。